{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Clase para procesar texto y extrar el vocabulario existente para su posterior mapeo.\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token\n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "\n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token\n",
    "\n",
    "        Args:\n",
    "            token (str): the token to look up\n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "\n",
    "        Args:\n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModsVocabulary(object):\n",
    "    \"\"\"Clase para procesar texto y extrar el vocabulario existente para su posterior mapeo.\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, unk_token='<UNK>', mask_token=\"<MASK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token\n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx,\n",
    "                'mask_token': self._mask_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "\n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token\n",
    "          or the UNK index if token isn't present.\n",
    "\n",
    "        Args:\n",
    "            token (str): the token to look up\n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary)\n",
    "              for the UNK functionality\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "\n",
    "        Args:\n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<ModsVocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basado en Rao, D., & McMahan, B. (2019). Natural language processing with PyTorch: build intelligent language applications using deep learning\n",
    "\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Vectorizer(object):\n",
    "    \"\"\"The Vectorizer coordinates the Vocabularies and puts them to use.\n",
    "    \"\"\"\n",
    "    def __init__(self, mods_vocab, personality_vocab):\n",
    "        \"\"\"\n",
    "        Args: mods_vocab: Maps words to Integers.\n",
    "            : personality_vocab: Maps class labels to Integers.\n",
    "        \"\"\"\n",
    "        self.mods_vocab = mods_vocab\n",
    "        self.personality_vocab = personality_vocab\n",
    "\n",
    "    def vectorize(self, mods, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mods (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        Returns:\n",
    "            the vetorized mods (numpy.array)\n",
    "        \"\"\"\n",
    "        indices = []\n",
    "        indices.extend(self.mods_vocab.lookup_token(token)\n",
    "                       for token in mods.split(\" \"))\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.mods_vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, char_df, cutoff=1):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "\n",
    "        Args:\n",
    "            char_df (pandas.DataFrame): the target dataset\n",
    "            cutoff (int): frequency threshold for including in Vocabulary\n",
    "        Returns:\n",
    "            an instance of the Vectorizer\n",
    "        \"\"\"\n",
    "        personality_vocab = Vocabulary()\n",
    "        for personality in sorted(set(char_df.target)):\n",
    "            personality_vocab.add_token(personality)\n",
    "\n",
    "        word_counts = Counter()\n",
    "        for mods in char_df.mods:\n",
    "            for token in mods.split(\" \"):\n",
    "                if token not in string.punctuation:\n",
    "                    word_counts[token] += 1\n",
    "\n",
    "        mods_vocab = ModsVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                mods_vocab.add_token(word)\n",
    "\n",
    "        return cls(mods_vocab, personality_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        mods_vocab = \\\n",
    "            ModsVocabulary.from_serializable(contents['mods_vocab'])\n",
    "        personality_vocab =  \\\n",
    "            Vocabulary.from_serializable(contents['personality_vocab'])\n",
    "\n",
    "        return cls(mods_vocab=mods_vocab, personality_vocab=personality_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'mods_vocab': self.mods_vocab.to_serializable(),\n",
    "                'personality_vocab': self.personality_vocab.to_serializable()}\n",
    "\n",
    "\n",
    "    def get_mods_vocab(self):\n",
    "        \"\"\"Returns the char Vocabulary.\n",
    "        \"\"\"\n",
    "        return self.mods_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, char_df, vectorizer):\n",
    "        '''\n",
    "        Args:\n",
    "            char_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (Vectorizer): vectorizer instatiated from dataset\n",
    "        '''\n",
    "        self.char_df = char_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, char_df.mods))\n",
    "\n",
    "\n",
    "        self.train_df = self.char_df[self.char_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.char_df[self.char_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.char_df[self.char_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                            'val': (self.val_df, self.validation_size),\n",
    "                            'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights\n",
    "        class_counts = char_df.target.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.personality_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, char_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "\n",
    "        Args:\n",
    "            char_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        char_df = pd.read_csv(char_csv)\n",
    "        train_char_df = char_df[char_df.split=='train']\n",
    "        return cls(char_df, Vectorizer.from_dataframe(train_char_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, char_csv, vectorizer_filepath):\n",
    "        \"\"\"Load dataset and the corresponding vectorizer.\n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "\n",
    "        Args:\n",
    "            char_csv (str): location of the dataset\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        char_csv = pd.read_csv(char_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(char_csv, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of Vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return Vectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "\n",
    "        Args:\n",
    "            index (int): the index to the data point\n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        mods_vector = \\\n",
    "            self._vectorizer.vectorize(row.mods, self._max_seq_length)\n",
    "\n",
    "        personality_index = \\\n",
    "            self._vectorizer.personality_vocab.lookup_token(row.target)\n",
    "\n",
    "        return {'x_data': mods_vector,\n",
    "                'y_target': personality_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "\n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                    drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will\n",
    "    ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    print(\"Hola\")\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "    print(\"Hola\")\n",
    "    print(dataloader)\n",
    "    for data_dict in dataloader:\n",
    "        print(data_dict)\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Guardamos por lo menos el modelo del primer epoch\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # En el resto de epochs...\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # El loss ha empeorado\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # El loss ha mejorado\n",
    "        else:\n",
    "            # Guardamos el mejor modelo\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El token que índice 8 es able.\n",
      "El índice del unknown word es 1.\n",
      "El objeto vocabulario de mods tiene un size de <ModsVocabulary(size=3863)>.\n",
      "El objeto vocabulario de mods tiene un size de <ModsVocabulary(size=3864)>.\n",
      "El objeto vocabulario de personalidades tiene un size de <Vocabulary(size=16)>.\n"
     ]
    }
   ],
   "source": [
    "#TODO: Reemplaza el path por el tuyo.\n",
    "dataset = CharDataset.load_dataset_and_make_vectorizer('../../../Datasets/dataset_clasificador_final/classify_char_raw.csv')\n",
    "#Respuestas a las preguntas\n",
    "mods_vocab = dataset.get_vectorizer().get_mods_vocab()\n",
    "personality_vocab = dataset.get_vectorizer().personality_vocab\n",
    "print(f\"El token que índice 8 es {mods_vocab.lookup_index(8)}.\")\n",
    "print(f\"El índice del unknown word es {mods_vocab.lookup_token('')}.\")\n",
    "print(f\"El objeto vocabulario de mods tiene un size de {mods_vocab}.\")\n",
    "#Añadimos un nuevo token al vocabulario por lo que deberíamos ver que el tamaño ha aumentado.\n",
    "mods_vocab.add_token(\"deusto\")\n",
    "print(f\"El objeto vocabulario de mods tiene un size de {mods_vocab}.\")\n",
    "print(f\"El objeto vocabulario de personalidades tiene un size de {personality_vocab}.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PersonalityClassifier(nn.Module):\n",
    "    \"\"\"Perceptron based Classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_size, num_embeddings, hidden_dim, num_features, num_classes, dropout_p,\n",
    "                 pretrained_embeddings=None, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            embedding_size (int): size of the embedding vectors\n",
    "            num_embeddings (int): number of embedding vectors\n",
    "            hidden_dim (int): the size of the hidden dimension\n",
    "            num_features(int): The size of input feature vector.\n",
    "            num_classes (int): the number of classes in classification\n",
    "            dropout_p (float): a dropout parameter\n",
    "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
    "                default is None. If provided,\n",
    "            padding_idx (int): an index representing a null position\n",
    "        \"\"\"\n",
    "        super(PersonalityClassifier, self).__init__()\n",
    "                \n",
    "        if pretrained_embeddings is None:\n",
    "            #En caso de no usar embeddings pre-entrenados, se inicializan usando una distribución normal con media 0 y desviación estándar 1.\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
    "                                    num_embeddings=num_embeddings,\n",
    "                                    padding_idx=padding_idx)\n",
    "        else:\n",
    "            #Usamos embvddings pre-entrenados.\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
    "                                    num_embeddings=num_embeddings,\n",
    "                                    padding_idx=padding_idx,\n",
    "                                    _weight=pretrained_embeddings)    \n",
    "\n",
    "        self._dropout_p = dropout_p\n",
    "        self.fc1 = nn.Linear(num_features, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "\n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor.\n",
    "                x_in.shape should be (batch, dataset._max_seq_length)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        # self.emb(x_in) transforma el indice de cada una de las 20 palabras a su correspondiendo word embedding --> (batch_size, sequence_length, embedding_dim).\n",
    "        # (batch_size, sequence_length, embedding_dim)\n",
    "        x_embedded = self.emb(x_in).permute(0, 2, 1)\n",
    "\n",
    "        # average and remove the extra dimension\n",
    "        remaining_size = x_embedded.size(dim=2)\n",
    "        features = F.avg_pool1d(x_embedded, remaining_size).squeeze(dim=2)\n",
    "        features = F.dropout(features, p=self._dropout_p)\n",
    "\n",
    "        # mlp classifier\n",
    "        intermediate_vector = F.relu(F.dropout(self.fc1(features), p=self._dropout_p))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will\n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\theal\\miniconda3\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def download_embedding_file(repo_id, filename, local_dir ):\n",
    "    if not os.path.exists(local_dir + filename):\n",
    "        print(\"Descargando los word embeddings\")\n",
    "        hf_hub_download(repo_id=repo_id, filename=filename, local_dir=local_dir)\n",
    "    else:\n",
    "        print(\"El fichero de embeddings ya está descargado.\")\n",
    "\n",
    "\n",
    "def get_embedding_file(embeddings_folder, filename):\n",
    "    word2vec_model = Word2Vec.load(embeddings_folder + filename)\n",
    "    # model = KeyedVectors.load_word2vec_format(embeddings_folder + filename, binary=True)\n",
    "    return word2vec_model.wv\n",
    "\n",
    "\n",
    "def make_embedding_matrix( local_dir, filename, words_idx_to_token, emb_dim):\n",
    "    embeddings_gensim = get_embedding_file(local_dir, filename)\n",
    "    final_embeddings = np.zeros((len(words_idx_to_token), emb_dim))\n",
    "    for idx in sorted(words_idx_to_token):\n",
    "        if words_idx_to_token[idx] in embeddings_gensim.key_to_index:\n",
    "            final_embeddings[idx, :] = embeddings_gensim[words_idx_to_token[idx]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, emb_dim)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[idx, :] = embedding_i\n",
    "            print(f\"Word {words_idx_to_token[idx]} not in model\")\n",
    "\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    char_csv=\"../../../Datasets/dataset_clasificador_final/classify_char_raw.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model_storage/model.pth\",\n",
    "    save_dir=\"model_storage/document_classification\",\n",
    "    # Model hyper parameters\n",
    "    use_emb=True,\n",
    "    repo_id = \"Word2vec/wikipedia2vec_enwiki_20180420_100d\", #Alternativa: \"Word2vec/wikipedia2vec_enwiki_20180420_300d\"\n",
    "    filename = \"combined_embeddings\", #Alternativa: \"enwiki_20180420_300d.txt\"\n",
    "    local_dir=\"../../../Embeddings/Word2Vec/\",\n",
    "    embedding_size=120, #Depende del modelo de word embeddings que usemos. Alternativa: 300\n",
    "    hidden_dim=100,\n",
    "    # Training hyper parameter\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    dropout_p=0.1,\n",
    "    batch_size=16,\n",
    "    num_epochs=200,\n",
    "    early_stopping_criteria=50,\n",
    "    # Runtime option\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n",
      "<ModsVocabulary(size=3863)>\n",
      "Word <MASK> not in model\n",
      "Word <UNK> not in model\n",
      "Word youll not in model\n",
      "Word theyre not in model\n",
      "Word persons not in model\n",
      "Word isnt not in model\n",
      "Word arent not in model\n",
      "Word whats not in model\n",
      "Word im not in model\n",
      "Word heres not in model\n",
      "Word thats not in model\n",
      "Word youre not in model\n",
      "Word lifes not in model\n",
      "Word dont not in model\n",
      "Word havent not in model\n",
      "Word anyones not in model\n",
      "Word elses not in model\n",
      "Word youve not in model\n",
      "Word doesnt not in model\n",
      "Word theyll not in model\n",
      "Word responsibilities not in model\n",
      "Word couldnt not in model\n",
      "Word theyve not in model\n",
      "Word wed not in model\n",
      "Word weve not in model\n",
      "Word theres not in model\n",
      "Word misunderstandings not in model\n",
      "Word theyd not in model\n",
      "Word wouldnt not in model\n",
      "Word devils not in model\n",
      "Word characteristically not in model\n",
      "Word ca not in model\n",
      "Word donts not in model\n",
      "Word wont not in model\n",
      "Word werent not in model\n",
      "Word societys not in model\n",
      "Word someones not in model\n",
      "Word enthusiastically not in model\n",
      "Word historys not in model\n",
      "Word whos not in model\n",
      "Word underappreciated not in model\n",
      "Word shouldnt not in model\n",
      "Word counterintuitive not in model\n",
      "Word eagles not in model\n",
      "Word uncharacteristic not in model\n",
      "Word id not in model\n",
      "Word unpredictability not in model\n",
      "Word didnt not in model\n",
      "Using pre-trained embeddings\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# create dataset and vectorizer\n",
    "dataset = CharDataset.load_dataset_and_make_vectorizer(args.char_csv)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "if args.use_emb:\n",
    "    mods_vocab = dataset.get_vectorizer().mods_vocab\n",
    "    print(mods_vocab)\n",
    "    repo_id = args.repo_id\n",
    "    filename = args.filename\n",
    "    local_dir = args.local_dir\n",
    "    embeddings = make_embedding_matrix(local_dir, filename, mods_vocab._idx_to_token, args.embedding_size)\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = PersonalityClassifier(embedding_size=args.embedding_size,\n",
    "                            num_embeddings=len(vectorizer.mods_vocab),\n",
    "                            hidden_dim=args.hidden_dim,\n",
    "                            num_features=120,\n",
    "                            num_classes=len(vectorizer.personality_vocab),\n",
    "                            dropout_p=args.dropout_p,\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "PersonalityClassifier                    [16, 16]                  --\n",
      "├─Embedding: 1-1                         [16, 20, 120]             463,560\n",
      "├─Linear: 1-2                            [16, 100]                 12,100\n",
      "├─Linear: 1-3                            [16, 16]                  1,616\n",
      "==========================================================================================\n",
      "Total params: 477,276\n",
      "Trainable params: 477,276\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 7.64\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.32\n",
      "Params size (MB): 1.91\n",
      "Estimated Total Size (MB): 2.23\n",
      "==========================================================================================\n",
      "In epoch 0 el vall_acc es 13.541666666666668 y el val_loss es 2.669593036174774\n",
      "Early stopping state is 0\n",
      "In epoch 1 el vall_acc es 20.833333333333332 y el val_loss es 2.5838974118232727\n",
      "Early stopping state is 0\n",
      "In epoch 2 el vall_acc es 23.958333333333332 y el val_loss es 2.487084805965424\n",
      "Early stopping state is 0\n",
      "In epoch 3 el vall_acc es 24.479166666666668 y el val_loss es 2.409114181995392\n",
      "Early stopping state is 0\n",
      "In epoch 4 el vall_acc es 26.041666666666664 y el val_loss es 2.3419444561004643\n",
      "Early stopping state is 0\n",
      "In epoch 5 el vall_acc es 29.6875 y el val_loss es 2.2000739077727\n",
      "Early stopping state is 0\n",
      "In epoch 6 el vall_acc es 30.208333333333332 y el val_loss es 2.159446845451991\n",
      "Early stopping state is 0\n",
      "In epoch 7 el vall_acc es 28.645833333333336 y el val_loss es 2.149012267589569\n",
      "Early stopping state is 0\n",
      "In epoch 8 el vall_acc es 29.166666666666664 y el val_loss es 2.2029769917329154\n",
      "Early stopping state is 1\n",
      "In epoch 9 el vall_acc es 33.854166666666664 y el val_loss es 2.155222753683726\n",
      "Early stopping state is 2\n",
      "In epoch 10 el vall_acc es 34.89583333333334 y el val_loss es 2.132755885521571\n",
      "Early stopping state is 0\n",
      "In epoch 11 el vall_acc es 34.895833333333336 y el val_loss es 2.1360024015108743\n",
      "Early stopping state is 1\n",
      "In epoch 12 el vall_acc es 30.729166666666668 y el val_loss es 2.160287280877431\n",
      "Early stopping state is 2\n",
      "In epoch 13 el vall_acc es 29.687499999999996 y el val_loss es 2.1800737778345747\n",
      "Early stopping state is 3\n",
      "In epoch 14 el vall_acc es 30.729166666666664 y el val_loss es 2.194293816884359\n",
      "Early stopping state is 4\n",
      "In epoch 15 el vall_acc es 34.895833333333336 y el val_loss es 2.189473787943522\n",
      "Early stopping state is 5\n",
      "In epoch 16 el vall_acc es 36.979166666666664 y el val_loss es 2.1273515025774636\n",
      "Early stopping state is 0\n",
      "In epoch 17 el vall_acc es 33.333333333333336 y el val_loss es 2.10503359635671\n",
      "Early stopping state is 0\n",
      "In epoch 18 el vall_acc es 32.291666666666664 y el val_loss es 2.20989066362381\n",
      "Early stopping state is 1\n",
      "In epoch 19 el vall_acc es 28.645833333333336 y el val_loss es 2.25392135977745\n",
      "Early stopping state is 2\n",
      "In epoch 20 el vall_acc es 34.895833333333336 y el val_loss es 2.15079269806544\n",
      "Early stopping state is 3\n",
      "In epoch 21 el vall_acc es 30.729166666666668 y el val_loss es 2.2323069175084433\n",
      "Early stopping state is 4\n",
      "In epoch 22 el vall_acc es 31.77083333333333 y el val_loss es 2.192372292280197\n",
      "Early stopping state is 5\n",
      "In epoch 23 el vall_acc es 29.166666666666664 y el val_loss es 2.1865750650564832\n",
      "Early stopping state is 6\n",
      "In epoch 24 el vall_acc es 32.291666666666664 y el val_loss es 2.1594031949838004\n",
      "Early stopping state is 7\n",
      "In epoch 25 el vall_acc es 33.33333333333333 y el val_loss es 2.097669710715612\n",
      "Early stopping state is 0\n",
      "In epoch 26 el vall_acc es 37.5 y el val_loss es 2.1085687975088754\n",
      "Early stopping state is 1\n",
      "In epoch 27 el vall_acc es 34.375 y el val_loss es 2.119821767012278\n",
      "Early stopping state is 2\n",
      "In epoch 28 el vall_acc es 36.97916666666667 y el val_loss es 2.133529742558797\n",
      "Early stopping state is 3\n",
      "In epoch 29 el vall_acc es 34.375 y el val_loss es 2.2638982137044272\n",
      "Early stopping state is 4\n",
      "In epoch 30 el vall_acc es 32.8125 y el val_loss es 2.190430223941803\n",
      "Early stopping state is 5\n",
      "In epoch 31 el vall_acc es 32.291666666666664 y el val_loss es 2.2293801903724675\n",
      "Early stopping state is 6\n",
      "In epoch 32 el vall_acc es 30.729166666666668 y el val_loss es 2.111563930908839\n",
      "Early stopping state is 7\n",
      "In epoch 33 el vall_acc es 32.291666666666664 y el val_loss es 2.1825700600941973\n",
      "Early stopping state is 8\n",
      "In epoch 34 el vall_acc es 33.33333333333333 y el val_loss es 2.198522577683131\n",
      "Early stopping state is 9\n",
      "In epoch 35 el vall_acc es 31.25 y el val_loss es 2.2146809001763663\n",
      "Early stopping state is 10\n",
      "In epoch 36 el vall_acc es 31.770833333333332 y el val_loss es 2.176353543996811\n",
      "Early stopping state is 11\n",
      "In epoch 37 el vall_acc es 38.541666666666664 y el val_loss es 2.1677372256914778\n",
      "Early stopping state is 12\n",
      "In epoch 38 el vall_acc es 32.8125 y el val_loss es 2.1797304848829904\n",
      "Early stopping state is 13\n",
      "In epoch 39 el vall_acc es 33.333333333333336 y el val_loss es 2.2127638459205627\n",
      "Early stopping state is 14\n",
      "In epoch 40 el vall_acc es 32.8125 y el val_loss es 2.1471093297004704\n",
      "Early stopping state is 15\n",
      "In epoch 41 el vall_acc es 33.33333333333333 y el val_loss es 2.2135392526785536\n",
      "Early stopping state is 16\n",
      "In epoch 42 el vall_acc es 32.8125 y el val_loss es 2.1650531093279524\n",
      "Early stopping state is 17\n",
      "In epoch 43 el vall_acc es 34.375 y el val_loss es 2.2422186632951098\n",
      "Early stopping state is 18\n",
      "In epoch 44 el vall_acc es 35.9375 y el val_loss es 2.12696428100268\n",
      "Early stopping state is 19\n",
      "In epoch 45 el vall_acc es 33.854166666666664 y el val_loss es 2.194434275229772\n",
      "Early stopping state is 20\n",
      "In epoch 46 el vall_acc es 35.9375 y el val_loss es 2.2008550067742663\n",
      "Early stopping state is 21\n",
      "In epoch 47 el vall_acc es 34.895833333333336 y el val_loss es 2.1068695088227587\n",
      "Early stopping state is 22\n",
      "In epoch 48 el vall_acc es 29.687499999999996 y el val_loss es 2.1852059264977775\n",
      "Early stopping state is 23\n",
      "In epoch 49 el vall_acc es 30.208333333333336 y el val_loss es 2.295631895462672\n",
      "Early stopping state is 24\n",
      "In epoch 50 el vall_acc es 33.85416666666667 y el val_loss es 2.1423499186833705\n",
      "Early stopping state is 25\n",
      "In epoch 51 el vall_acc es 35.416666666666664 y el val_loss es 2.1393511394659677\n",
      "Early stopping state is 26\n",
      "In epoch 52 el vall_acc es 32.291666666666664 y el val_loss es 2.1480805079142256\n",
      "Early stopping state is 27\n",
      "In epoch 53 el vall_acc es 29.16666666666667 y el val_loss es 2.267418841520945\n",
      "Early stopping state is 28\n",
      "In epoch 54 el vall_acc es 32.8125 y el val_loss es 2.1599965890248614\n",
      "Early stopping state is 29\n",
      "In epoch 55 el vall_acc es 31.77083333333333 y el val_loss es 2.166202982266744\n",
      "Early stopping state is 30\n",
      "In epoch 56 el vall_acc es 31.77083333333334 y el val_loss es 2.1896811823050184\n",
      "Early stopping state is 31\n",
      "In epoch 57 el vall_acc es 36.45833333333332 y el val_loss es 2.175846050182978\n",
      "Early stopping state is 32\n",
      "In epoch 58 el vall_acc es 32.291666666666664 y el val_loss es 2.121308396259944\n",
      "Early stopping state is 33\n",
      "In epoch 59 el vall_acc es 33.33333333333333 y el val_loss es 2.2367760340372724\n",
      "Early stopping state is 34\n",
      "In epoch 60 el vall_acc es 33.854166666666664 y el val_loss es 2.1473690966765084\n",
      "Early stopping state is 35\n",
      "In epoch 61 el vall_acc es 33.333333333333336 y el val_loss es 2.189011345307032\n",
      "Early stopping state is 36\n",
      "In epoch 62 el vall_acc es 31.25 y el val_loss es 2.1907349924246473\n",
      "Early stopping state is 37\n",
      "In epoch 63 el vall_acc es 31.25 y el val_loss es 2.273303379615148\n",
      "Early stopping state is 38\n",
      "In epoch 64 el vall_acc es 30.20833333333333 y el val_loss es 2.22033150990804\n",
      "Early stopping state is 39\n",
      "In epoch 65 el vall_acc es 33.33333333333333 y el val_loss es 2.169195681810379\n",
      "Early stopping state is 40\n",
      "In epoch 66 el vall_acc es 34.89583333333333 y el val_loss es 2.136754264434179\n",
      "Early stopping state is 41\n",
      "In epoch 67 el vall_acc es 35.9375 y el val_loss es 2.1325392425060268\n",
      "Early stopping state is 42\n",
      "In epoch 68 el vall_acc es 32.8125 y el val_loss es 2.2129476368427277\n",
      "Early stopping state is 43\n",
      "In epoch 69 el vall_acc es 34.895833333333336 y el val_loss es 2.1368006964524584\n",
      "Early stopping state is 44\n",
      "In epoch 70 el vall_acc es 32.291666666666664 y el val_loss es 2.203569134076436\n",
      "Early stopping state is 45\n",
      "In epoch 71 el vall_acc es 35.416666666666664 y el val_loss es 2.1207258502642317\n",
      "Early stopping state is 46\n",
      "In epoch 72 el vall_acc es 27.60416666666667 y el val_loss es 2.2496713300546007\n",
      "Early stopping state is 47\n",
      "In epoch 73 el vall_acc es 32.291666666666664 y el val_loss es 2.233475397030513\n",
      "Early stopping state is 48\n",
      "In epoch 74 el vall_acc es 35.416666666666664 y el val_loss es 2.1808222830295563\n",
      "Early stopping state is 49\n",
      "In epoch 75 el vall_acc es 33.85416666666667 y el val_loss es 2.1138945917288465\n",
      "Early stopping state is 50\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "summary_shown = False\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            if not summary_shown:\n",
    "                print(summary(classifier, input_data=[batch_dict['x_data']]))\n",
    "                summary_shown = True\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # compute the output\n",
    "            y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "        print(f\"In epoch {epoch_index} el vall_acc es {running_acc} y el val_loss es {running_loss}\" )\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "        print(f\"Early stopping state is {train_state['early_stopping_step']}\")\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.073670208454132;\n",
      "Test Accuracy: 34.375\n"
     ]
    }
   ],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset,\n",
    "                                   batch_size=args.batch_size,\n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n",
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
