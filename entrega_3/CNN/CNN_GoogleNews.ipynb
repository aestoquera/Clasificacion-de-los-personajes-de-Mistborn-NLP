{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Clase para procesar texto y extrar el vocabulario existente para su posterior mapeo.\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token\n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "\n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token\n",
    "\n",
    "        Args:\n",
    "            token (str): the token to look up\n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "\n",
    "        Args:\n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModsVocabulary(object):\n",
    "    \"\"\"Clase para procesar texto y extrar el vocabulario existente para su posterior mapeo.\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, unk_token='<UNK>', mask_token=\"<MASK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token\n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx,\n",
    "                'mask_token': self._mask_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "\n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token\n",
    "          or the UNK index if token isn't present.\n",
    "\n",
    "        Args:\n",
    "            token (str): the token to look up\n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary)\n",
    "              for the UNK functionality\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "\n",
    "        Args:\n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<ModsVocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basado en Rao, D., & McMahan, B. (2019). Natural language processing with PyTorch: build intelligent language applications using deep learning\n",
    "\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Vectorizer(object):\n",
    "    \"\"\"The Vectorizer coordinates the Vocabularies and puts them to use.\n",
    "    \"\"\"\n",
    "    def __init__(self, mods_vocab, personality_vocab):\n",
    "        \"\"\"\n",
    "        Args: mods_vocab: Maps words to Integers.\n",
    "            : personality_vocab: Maps class labels to Integers.\n",
    "        \"\"\"\n",
    "        self.mods_vocab = mods_vocab\n",
    "        self.personality_vocab = personality_vocab\n",
    "\n",
    "    def vectorize(self, mods, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mods (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        Returns:\n",
    "            the vetorized mods (numpy.array)\n",
    "        \"\"\"\n",
    "        indices = []\n",
    "        indices.extend(self.mods_vocab.lookup_token(token)\n",
    "                       for token in mods.split(\" \"))\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.mods_vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, char_df, cutoff=1):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "\n",
    "        Args:\n",
    "            char_df (pandas.DataFrame): the target dataset\n",
    "            cutoff (int): frequency threshold for including in Vocabulary\n",
    "        Returns:\n",
    "            an instance of the Vectorizer\n",
    "        \"\"\"\n",
    "        personality_vocab = Vocabulary()\n",
    "        for personality in sorted(set(char_df.target)):\n",
    "            personality_vocab.add_token(personality)\n",
    "\n",
    "        word_counts = Counter()\n",
    "        for mods in char_df.mods:\n",
    "            for token in mods.split(\" \"):\n",
    "                if token not in string.punctuation:\n",
    "                    word_counts[token] += 1\n",
    "\n",
    "        mods_vocab = ModsVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                mods_vocab.add_token(word)\n",
    "\n",
    "        return cls(mods_vocab, personality_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        mods_vocab = \\\n",
    "            ModsVocabulary.from_serializable(contents['mods_vocab'])\n",
    "        personality_vocab =  \\\n",
    "            Vocabulary.from_serializable(contents['personality_vocab'])\n",
    "\n",
    "        return cls(mods_vocab=mods_vocab, personality_vocab=personality_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'mods_vocab': self.mods_vocab.to_serializable(),\n",
    "                'personality_vocab': self.personality_vocab.to_serializable()}\n",
    "\n",
    "\n",
    "    def get_mods_vocab(self):\n",
    "        \"\"\"Returns the char Vocabulary.\n",
    "        \"\"\"\n",
    "        return self.mods_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, char_df, vectorizer):\n",
    "        '''\n",
    "        Args:\n",
    "            char_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (Vectorizer): vectorizer instatiated from dataset\n",
    "        '''\n",
    "        self.char_df = char_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, char_df.mods))\n",
    "\n",
    "\n",
    "        self.train_df = self.char_df[self.char_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.char_df[self.char_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.char_df[self.char_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                            'val': (self.val_df, self.validation_size),\n",
    "                            'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights\n",
    "        class_counts = char_df.target.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.personality_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, char_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "\n",
    "        Args:\n",
    "            char_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        char_df = pd.read_csv(char_csv)\n",
    "        train_char_df = char_df[char_df.split=='train']\n",
    "        return cls(char_df, Vectorizer.from_dataframe(train_char_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, char_csv, vectorizer_filepath):\n",
    "        \"\"\"Load dataset and the corresponding vectorizer.\n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "\n",
    "        Args:\n",
    "            char_csv (str): location of the dataset\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        char_csv = pd.read_csv(char_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(char_csv, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of Vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return Vectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "\n",
    "        Args:\n",
    "            index (int): the index to the data point\n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        mods_vector = \\\n",
    "            self._vectorizer.vectorize(row.mods, self._max_seq_length)\n",
    "\n",
    "        personality_index = \\\n",
    "            self._vectorizer.personality_vocab.lookup_token(row.target)\n",
    "\n",
    "        return {'x_data': mods_vector,\n",
    "                'y_target': personality_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "\n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                    drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will\n",
    "    ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    print(\"Hola\")\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "    print(\"Hola\")\n",
    "    print(dataloader)\n",
    "    for data_dict in dataloader:\n",
    "        print(data_dict)\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Guardamos por lo menos el modelo del primer epoch\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # En el resto de epochs...\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # El loss ha empeorado\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # El loss ha mejorado\n",
    "        else:\n",
    "            # Guardamos el mejor modelo\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El token que índice 8 es able.\n",
      "El índice del unknown word es 1.\n",
      "El objeto vocabulario de mods tiene un size de <ModsVocabulary(size=3863)>.\n",
      "El objeto vocabulario de mods tiene un size de <ModsVocabulary(size=3864)>.\n",
      "El objeto vocabulario de personalidades tiene un size de <Vocabulary(size=16)>.\n"
     ]
    }
   ],
   "source": [
    "#TODO: Reemplaza el path por el tuyo.\n",
    "dataset = CharDataset.load_dataset_and_make_vectorizer('../../../Datasets/dataset_clasificador_final/classify_char_raw.csv')\n",
    "#Respuestas a las preguntas\n",
    "mods_vocab = dataset.get_vectorizer().get_mods_vocab()\n",
    "personality_vocab = dataset.get_vectorizer().personality_vocab\n",
    "print(f\"El token que índice 8 es {mods_vocab.lookup_index(8)}.\")\n",
    "print(f\"El índice del unknown word es {mods_vocab.lookup_token('')}.\")\n",
    "print(f\"El objeto vocabulario de mods tiene un size de {mods_vocab}.\")\n",
    "#Añadimos un nuevo token al vocabulario por lo que deberíamos ver que el tamaño ha aumentado.\n",
    "mods_vocab.add_token(\"deusto\")\n",
    "print(f\"El objeto vocabulario de mods tiene un size de {mods_vocab}.\")\n",
    "print(f\"El objeto vocabulario de personalidades tiene un size de {personality_vocab}.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_NLP(nn.Module):\n",
    "    \"\"\"An 1D Convulational Neural Network for Sentence Classification.\n",
    "    Adapted from: https://www.kaggle.com/code/williamlwcliu/cnn-text-classification-pytorch\"\"\"\n",
    "    def __init__(self,\n",
    "                 pretrained_embedding=None,\n",
    "                 freeze_embedding=False,\n",
    "                 vocab_size=None,\n",
    "                 embed_dim=100,\n",
    "                 filter_sizes=[3, 4, 5],\n",
    "                 num_filters=[100, 100, 100],\n",
    "                 num_classes=4,\n",
    "                 dropout=0.5):\n",
    "        \"\"\"\n",
    "        The constructor for CNN_NLP class.\n",
    "\n",
    "        Args:\n",
    "            pretrained_embedding (torch.Tensor): Pretrained embeddings with\n",
    "                shape (vocab_size, embed_dim)\n",
    "            freeze_embedding (bool): Set to False to fine-tune pretraiend\n",
    "                vectors. Default: False\n",
    "            vocab_size (int): Need to be specified when not pretrained word\n",
    "                embeddings are not used.\n",
    "            embed_dim (int): Dimension of word vectors. Need to be specified\n",
    "                when pretrained word embeddings are not used. Default: 300\n",
    "            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n",
    "            num_filters (List[int]): List of number of filters, has the same\n",
    "                length as `filter_sizes`. Default: [100, 100, 100]\n",
    "            n_classes (int): Number of classes. Default: 2\n",
    "            dropout (float): Dropout rate. Default: 0.5\n",
    "        \"\"\"\n",
    "\n",
    "        super(CNN_NLP, self).__init__()\n",
    "        # Embedding layer\n",
    "        if pretrained_embedding is not None:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embedding).float()\n",
    "            self.emb = nn.Embedding(embedding_dim=embed_dim,\n",
    "                                    num_embeddings=vocab_size,\n",
    "                                    padding_idx=0,\n",
    "                                    _weight=pretrained_embeddings)\n",
    "        else:\n",
    "            self.embed_dim = embed_dim\n",
    "            self.emb = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                          embedding_dim=embed_dim,\n",
    "                                          padding_idx=0,\n",
    "                                          max_norm=5.0)\n",
    "        # Conv Network\n",
    "        self.conv1d_list = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_dim,\n",
    "                      out_channels=num_filters[i],\n",
    "                      kernel_size=filter_sizes[i])\n",
    "            for i in range(len(filter_sizes))\n",
    "        ])\n",
    "        # Fully-connected layer and Dropout\n",
    "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): A tensor of token ids with shape\n",
    "                (batch_size, max_sent_length)\n",
    "\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Output logits with shape (batch_size,\n",
    "                n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n",
    "        x_embed = self.emb(input_ids).float()\n",
    "\n",
    "        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n",
    "        # Output shape: (b, embed_dim, max_len)\n",
    "        x_reshaped = x_embed.permute(0, 2, 1)\n",
    "\n",
    "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
    "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
    "\n",
    "        # Max pooling. Output shape: (b, num_filters[i], 1)\n",
    "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
    "            for x_conv in x_conv_list]\n",
    "\n",
    "        # Concatenate x_pool_list to feed the fully connected layer.\n",
    "        # Output shape: (b, sum(num_filters))\n",
    "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
    "                         dim=1)\n",
    "\n",
    "        # Compute logits. Output shape: (b, n_classes)\n",
    "        logits = self.fc(self.dropout(x_fc))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will\n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\theal\\miniconda3\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def download_embedding_file(repo_id, filename, local_dir ):\n",
    "    if not os.path.exists(local_dir + filename):\n",
    "        print(\"Descargando los word embeddings\")\n",
    "        hf_hub_download(repo_id=repo_id, filename=filename, local_dir=local_dir)\n",
    "    else:\n",
    "        print(\"El fichero de embeddings ya está descargado.\")\n",
    "\n",
    "\n",
    "def get_embedding_file(embeddings_folder, filename):\n",
    "    model = KeyedVectors.load_word2vec_format(embeddings_folder + filename)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_embedding_matrix( local_dir, filename, words_idx_to_token, emb_dim):\n",
    "    embeddings_gensim = get_embedding_file(local_dir, filename)\n",
    "    final_embeddings = np.zeros((len(words_idx_to_token), emb_dim))\n",
    "    for idx in sorted(words_idx_to_token):\n",
    "        if words_idx_to_token[idx] in embeddings_gensim.key_to_index:\n",
    "            final_embeddings[idx, :] = embeddings_gensim[words_idx_to_token[idx]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, emb_dim)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[idx, :] = embedding_i\n",
    "            print(f\"Word {words_idx_to_token[idx]} not in model\")\n",
    "\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    char_csv=\"../../../Datasets/dataset_clasificador_final/classify_char_raw.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model_storage/model.pth\",\n",
    "    save_dir=\"model_storage/document_classification\",\n",
    "    # Model hyper parameters\n",
    "    use_emb=True,\n",
    "    repo_id = \"Word2vec/wikipedia2vec_enwiki_20180420_100d\", #Alternativa: \"Word2vec/wikipedia2vec_enwiki_20180420_300d\"\n",
    "    filename = \"enwiki_20180420_100d.txt\", #Alternativa: \"enwiki_20180420_300d.txt\"\n",
    "    local_dir=\"../../../Embeddings/Word2Vec/\",\n",
    "    embedding_size=100, #Depende del modelo de word embeddings que usemos. Alternativa: 300\n",
    "    hidden_dim=100,\n",
    "    num_channels=100, #AKA number of filters\n",
    "    # Training hyper parameter\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    dropout_p=0.1,\n",
    "    batch_size=16,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime option\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n",
      "<ModsVocabulary(size=3863)>\n",
      "Descargando los word embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "enwiki_20180420_100d.txt: 100%|██████████| 3.49G/3.49G [21:20<00:00, 2.73MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word <MASK> not in model\n",
      "Word <UNK> not in model\n",
      "Word crewleader not in model\n",
      "Word youve not in model\n",
      "Word theyll not in model\n",
      "Word pewterarm not in model\n",
      "Word obligator not in model\n",
      "Word theyve not in model\n",
      "Word theyd not in model\n",
      "Word tindwyl not in model\n",
      "Word buzzkills not in model\n",
      "Word speedreading not in model\n",
      "Word werent not in model\n",
      "Word shouldnt not in model\n",
      "Using pre-trained embeddings\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# create dataset and vectorizer\n",
    "dataset = CharDataset.load_dataset_and_make_vectorizer(args.char_csv)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "if args.use_emb:\n",
    "    mods_vocab = dataset.get_vectorizer().mods_vocab\n",
    "    print(mods_vocab)\n",
    "    repo_id = args.repo_id\n",
    "    filename = args.filename\n",
    "    local_dir = args.local_dir\n",
    "    download_embedding_file(repo_id, filename, local_dir)\n",
    "    embeddings = make_embedding_matrix(local_dir, filename, mods_vocab._idx_to_token, args.embedding_size)\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = CNN_NLP(pretrained_embedding=embeddings,\n",
    "                        freeze_embedding=False,\n",
    "                        vocab_size=len(vectorizer.mods_vocab),\n",
    "                        embed_dim=args.embedding_size,\n",
    "                        num_classes=len(vectorizer.personality_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "CNN_NLP                                  [16, 16]                  --\n",
      "├─Embedding: 1-1                         [16, 20, 100]             386,300\n",
      "├─ModuleList: 1-2                        --                        --\n",
      "│    └─Conv1d: 2-1                       [16, 100, 18]             30,100\n",
      "│    └─Conv1d: 2-2                       [16, 100, 17]             40,100\n",
      "│    └─Conv1d: 2-3                       [16, 100, 16]             50,100\n",
      "├─Dropout: 1-3                           [16, 300]                 --\n",
      "├─Linear: 1-4                            [16, 16]                  4,816\n",
      "==========================================================================================\n",
      "Total params: 511,416\n",
      "Trainable params: 511,416\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 38.66\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.91\n",
      "Params size (MB): 2.05\n",
      "Estimated Total Size (MB): 2.96\n",
      "==========================================================================================\n",
      "In epoch 0 el vall_acc es 5.208333333333333 y el val_loss es 2.766687492529551\n",
      "Early stopping state is 0\n",
      "In epoch 1 el vall_acc es 16.666666666666668 y el val_loss es 2.700713256994883\n",
      "Early stopping state is 0\n",
      "In epoch 2 el vall_acc es 15.625 y el val_loss es 2.6842758854230246\n",
      "Early stopping state is 0\n",
      "In epoch 3 el vall_acc es 17.1875 y el val_loss es 2.573392510414124\n",
      "Early stopping state is 0\n",
      "In epoch 4 el vall_acc es 21.875 y el val_loss es 2.47630234559377\n",
      "Early stopping state is 0\n",
      "In epoch 5 el vall_acc es 28.125000000000004 y el val_loss es 2.4043694933255515\n",
      "Early stopping state is 0\n",
      "In epoch 6 el vall_acc es 29.166666666666668 y el val_loss es 2.30463174978892\n",
      "Early stopping state is 0\n",
      "In epoch 7 el vall_acc es 30.729166666666668 y el val_loss es 2.221046894788742\n",
      "Early stopping state is 0\n",
      "In epoch 8 el vall_acc es 34.895833333333336 y el val_loss es 2.088070829709371\n",
      "Early stopping state is 0\n",
      "In epoch 9 el vall_acc es 33.854166666666664 y el val_loss es 2.08218319217364\n",
      "Early stopping state is 0\n",
      "In epoch 10 el vall_acc es 33.854166666666664 y el val_loss es 2.0772352914015455\n",
      "Early stopping state is 0\n",
      "In epoch 11 el vall_acc es 35.9375 y el val_loss es 1.977527012427648\n",
      "Early stopping state is 0\n",
      "In epoch 12 el vall_acc es 37.5 y el val_loss es 2.0033084849516554\n",
      "Early stopping state is 1\n",
      "In epoch 13 el vall_acc es 34.895833333333336 y el val_loss es 1.9796784023443856\n",
      "Early stopping state is 2\n",
      "In epoch 14 el vall_acc es 37.5 y el val_loss es 1.9513335824012756\n",
      "Early stopping state is 0\n",
      "In epoch 15 el vall_acc es 36.97916666666667 y el val_loss es 1.958268334468206\n",
      "Early stopping state is 1\n",
      "In epoch 16 el vall_acc es 37.50000000000001 y el val_loss es 1.91556919614474\n",
      "Early stopping state is 0\n",
      "In epoch 17 el vall_acc es 38.54166666666667 y el val_loss es 1.9209582308928175\n",
      "Early stopping state is 1\n",
      "In epoch 18 el vall_acc es 36.97916666666667 y el val_loss es 1.9497254788875578\n",
      "Early stopping state is 2\n",
      "In epoch 19 el vall_acc es 36.458333333333336 y el val_loss es 1.9363283912340798\n",
      "Early stopping state is 3\n",
      "In epoch 20 el vall_acc es 35.9375 y el val_loss es 1.943018396695455\n",
      "Early stopping state is 4\n",
      "In epoch 21 el vall_acc es 38.02083333333333 y el val_loss es 1.928305725256602\n",
      "Early stopping state is 5\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "summary_shown = False\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            if not summary_shown:\n",
    "                print(summary(classifier, input_data=[batch_dict['x_data']]))\n",
    "                summary_shown = True\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # compute the output\n",
    "            y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "        print(f\"In epoch {epoch_index} el vall_acc es {running_acc} y el val_loss es {running_loss}\" )\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "        print(f\"Early stopping state is {train_state['early_stopping_step']}\")\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.9080875118573508;\n",
      "Test Accuracy: 37.5\n"
     ]
    }
   ],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset,\n",
    "                                   batch_size=args.batch_size,\n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n",
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
